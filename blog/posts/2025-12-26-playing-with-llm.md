---
title_pl: "Tokenizery w modelach jÄ™zykowych - praktyczne porÃ³wnanie"
title_en: "Tokenizers in Language Models - A Practical Comparison"
date: 2025-12-26
author: "Marcin Piotrowski"
tags: ["NLP", "tokenization", "transformers", "LLM", "BERT", "GPT"]
description_pl: "Praktyczny przewodnik po tokenizerach w duÅ¼ych modelach jÄ™zykowych. PorÃ³wnanie dziaÅ‚ania tokenizers BERT, GPT-4, GPT-2, T5, StarCoder i XLM-RoBERTa na przykÅ‚adach wielojÄ™zycznych."
description_en: "A practical guide to tokenizers in large language models. Comparison of BERT, GPT-4, GPT-2, T5, StarCoder and XLM-RoBERTa tokenizers with multilingual examples."
---

## PL

## WstÄ™p

Tokenizer to jeden z najwaÅ¼niejszych, choÄ‡ czÄ™sto pomijanych komponentÃ³w kaÅ¼dego duÅ¼ego modelu jÄ™zykowego. Jego wybÃ³r ma bezpoÅ›redni wpÅ‚yw na wydajnoÅ›Ä‡ modelu, jakoÅ›Ä‡ wynikÃ³w oraz efektywnoÅ›Ä‡ przetwarzania tekstu. W tym artykule przyjrzymy siÄ™ praktycznemu dziaÅ‚aniu rÃ³Å¼nych tokenizerÃ³w i zobaczymy, jak radzÄ… sobie z wielojÄ™zycznymi tekstami, emoji i kodem ÅºrÃ³dÅ‚owym.

> Prezentowany materiaÅ‚ zostaÅ‚ opracowany w oparciu o wiedzÄ™ zdobytÄ… podczas krÃ³tkiego, darmowego kursu dostÄ™pnego na platformie deeplearning.ai: [How Transformer LLMs Work](https://www.deeplearning.ai/short-courses/how-transformer-llms-work/)
> **Kod ÅºrÃ³dÅ‚owy:** Wszystkie eksperymenty z tego artykuÅ‚u dostÄ™pne sÄ… w : [Google Colab - Tokenizer Comparison](https://colab.research.google.com/drive/1nuKOvO3WqcEySQeHeUEa4ZzzheRX7FFw?usp=sharing)


## Tokenizer - most miÄ™dzy czÅ‚owiekiem a modelem

Tokenizer stanowi punkt wejÅ›cia do kaÅ¼dego duÅ¼ego modelu jÄ™zykowego. MoÅ¼na powiedzieÄ‡, Å¼e stanowi most pomiÄ™dzy czÅ‚owiekiem a modelem, poniewaÅ¼ model nie operuje bezpoÅ›rednio na sÅ‚owach czy literach, lecz na tokenach. W praktyce czÄ™sto upraszcza siÄ™, Å¼e sÅ‚owo = token, lecz w rzeczywistoÅ›ci jedno sÅ‚owo moÅ¼e skÅ‚adaÄ‡ siÄ™ z wielu tokenÃ³w. 
KaÅ¼dy LLM posiada swÃ³j wÅ‚asny sÅ‚ownik tokenÃ³w - kaÅ¼dy token ma unikalne ID. Zadaniem tokenizera jest zamiana tekstu na ciÄ…g tokenÃ³w i przekazanie listy ich ID, aby model mÃ³gÅ‚ wykonaÄ‡ swojÄ… pracÄ™. 
W tym wpisie przybliÅ¼Ä™ dziaÅ‚anie rÃ³Å¼nych tokenizerÃ³w w praktyce i zaobserwujemy rÃ³Å¼nice miÄ™dzy nimi, nie wchodzÄ…c w szczegÃ³Å‚y techniczne. Wykorzystamy do tego celu API Hugging Face.

## Praktyczna demonstracja

Aby dokonaÄ‡ zamiany tekstu na tokeny, wystarczy kilka linii kodu:

```python
from transformers import AutoTokenizer
sentence = "Hello world!"
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokens = tokenizer(sentence)
```

`AutoTokenizer` to jedna z klas API Hugging Face, ktÃ³re udostÄ™pnia tysiÄ…ce modeli na zasadach open source. Na podstawie podanej nazwy modelu (w tym przypadku `bert-base-cased`) automatycznie:

1. Pobiera odpowiedni tokenizer z repozytorium Hugging Face
2. Åaduje jego konfiguracjÄ™ i sÅ‚ownik
3. Zapisuje go lokalnie w cache na przyszÅ‚oÅ›Ä‡

DziÄ™ki temu nie musimy rÄ™cznie sprawdzaÄ‡, jakiego konkretnie tokenizera uÅ¼yÄ‡ - `AutoTokenizer` rozpoznaje typ modelu i Å‚aduje wÅ‚aÅ›ciwÄ… implementacjÄ™.

### Struktura obiektu BatchEncoding

ZwrÃ³cony obiekt `tokens` jest instancjÄ… klasy `BatchEncoding`, ktÃ³ra implementuje interfejs sÅ‚ownikowy i zawiera nastÄ™pujÄ…ce komponenty:

| Klucz | Opis | PrzykÅ‚ad |
|-------|------|----------|
| `input_ids` | Sekwencja identyfikatorÃ³w tokenÃ³w | `[101, 8667, 1362, 106, 102]` |
| `attention_mask` | Maska wskazujÄ…ca rzeczywiste tokeny vs. padding | `[1, 1, 1, 1, 1]` |
| `token_type_ids` | Identyfikacja przynaleÅ¼noÅ›ci do segmentÃ³w (w zadaniach z parami zdaÅ„) | `[0, 0, 0, 0, 0]` |
```python
print(tokens)
# Output:
# {'input_ids': [101, 8667, 1362, 106, 102], 
#  'token_type_ids': [0, 0, 0, 0, 0], 
#  'attention_mask': [1, 1, 1, 1, 1]}
```


### Dekodowanie tokenÃ³w

Aby zdekodowaÄ‡ ID tokenÃ³w do konkretnych sÅ‚Ã³w, wystarczy uÅ¼yÄ‡ funkcji `decode`:

```python
for id in token_ids:
    print(tokenizer.decode(id))
```
```
[CLS]
Hello
world
!
[SEP]
```

W zdekodowanych tokenach poza sÅ‚owami widzimy tzw. **tokeny specjalne**, ktÃ³re majÄ… nastÄ™pujÄ…ce znaczenia:

- **`[CLS]`** (*classification*) - token inicjalizujÄ…cy sekwencjÄ™, wykorzystywany w zadaniach klasyfikacyjnych
- **`[SEP]`** (*separator*) - delimiter segmentujÄ…cy lub terminujÄ…cy sekwencjÄ™
- **`[UNK]`** (*unknown*) - reprezentacja tokenÃ³w nieobecnych w sÅ‚owniku
- **`[PAD]`** (*padding*) - wyrÃ³wnanie dÅ‚ugoÅ›ci sekwencji w batch'ach

PowyÅ¼szy przykÅ‚ad demonstruje operacje wykonywane przez kaÅ¼dy LLM podczas obsÅ‚ugi naszych zapytaÅ„. Najpierw wejÅ›ciowy prompt jest zamieniany na tokeny, nastÄ™pnie model przetwarza te tokeny, a na sam koniec sÄ… one dekodowane z powrotem do tekstu, aby uÅ¼ytkownik mÃ³gÅ‚ go przeczytaÄ‡.

## PorÃ³wnanie tokenizerÃ³w

Aby systematycznie przeanalizowaÄ‡ rÃ³Å¼nice w implementacjach tokenizerÃ³w, przygotowano tekst testowy zawierajÄ…cy wyzwania charakterystyczne dla przetwarzania jÄ™zyka naturalnego:

-  Teksty w jÄ™zyku angielskim z rÃ³Å¼nicowanÄ… wielkoÅ›ciÄ… liter
-  Emotikony i symbole Unicode (ğŸµ ğŸ¥¸ é¸Ÿ)
-  Fragmenty kodu ÅºrÃ³dÅ‚owego z operatorami logicznymi
-  Sekwencje biaÅ‚ych znakÃ³w (tabulatory, spacje)
-  WyraÅ¼enia numeryczne i matematyczne
-  Tekst w jÄ™zyku polskim ze znakami diakrytycznymi

```python
text = """
English and CAPITALIZATION
ğŸµ ğŸ¥¸  é¸Ÿ
show_tokens False None elif == >= else: two tabs:"    " Three tabs: "       "
12.0*50=600
PrzykÅ‚adowe zdanie w jÄ™zyku polskim, Å¼Ã³Å‚Ä‡
"""
```

#### Wyniki porÃ³wnania

### ğŸ”¹ BERT base-cased
**Charakterystyka:** Model BERT z zachowaniem wielkoÅ›ci liter, sÅ‚ownik: 28,996 tokenÃ³w
```
Vocab length: 28996
[CLS] English and CA ##PI ##TA ##L ##I ##Z ##AT ##ION [UNK] [UNK] [UNK] show _ token ##s F ##als ##e None el ##if = = > = else : two ta ##bs : " " Three ta ##bs : " " 12 . 0 * 50 = 600 P ##rz ##yk ##Å‚a ##do ##we z ##dan ##ie w j ##Ä™ ##zy ##ku p ##ols ##kim , Å¼ ##Ã³ ##Å‚ ##Ä‡ [SEP]
```
**Obserwacje:**
- Wykorzystanie prefiksu `##` do oznaczenia sub-tokenÃ³w (WordPiece)
- SÅ‚owa wielosylabowe zostaÅ‚y rozbite na liczne tokeny
- Brak wsparcia dla emoji â†’ tokeny `[UNK]`
- ObsÅ‚uguje polskie znaki diakrytyczne, lecz rozbija je na osobne tokeny 

### ğŸ”¹ BERT base-uncased
**Charakterystyka:** Wariant BERT z normalizacjÄ… do maÅ‚ych liter, sÅ‚ownik: 30,522 tokenÃ³w
```
ocab length: 30522
[CLS] english and capital ##ization [UNK] [UNK] [UNK] show _ token ##s false none eli ##f = = > = else : two tab ##s : " " three tab ##s : " " 12 . 0 * 50 = 600 pr ##zy ##k ##Å‚a ##do ##we z ##dan ##ie w je ##zy ##ku pol ##ski ##m , z ##o ##Å‚ ##c [SEP]
```
**Obserwacje:**
- CaÅ‚kowita utrata informacji o wielkoÅ›ci liter
- Nieznacznie wiÄ™kszy sÅ‚ownik niÅ¼ wersja *cased*
- Podobne problemy z reprezentacjÄ… znakÃ³w specjalnych, ponadto utrata czÄ™Å›ci informacji (Å¼ -> z)

### Xenova/gpt-4
**Charakterystyka:** Implementacja tokenizera GPT-4, sÅ‚ownik: 100,263 tokeny
```
Vocab length: 100263

 English  and  CAPITAL IZATION 
 ï¿½ ï¿½ ï¿½  ï¿½ ï¿½ ï¿½    ï¿½ ï¿½ ï¿½ 
 show _tokens  False  None  elif  ==  >=  else :  two  tabs :"      "  Three  tabs :  "         "
 12 . 0 * 50 = 600 
 Pr zy k Å‚ adow e  zd anie  w  j Ä™ zy ku  pol sk im ,  Å¼ Ã³Å‚ Ä‡ 
```
**Obserwacje:**
- ZnaczÄ…co wiÄ™kszy sÅ‚ownik umoÅ¼liwia bardziej efektywnÄ… tokenizacjÄ™, nie ma tak duÅ¼ej iloÅ›ci tokenÃ³w dla wielosylabowych sÅ‚Ã³w
- Lepsza obsÅ‚uga biaÅ‚ych znakÃ³w i struktury kodu
- Umiarkowane wsparcie dla jÄ™zyka polskiego, dalej rozbija polskie sÅ‚owa na wiele tokenÃ³w
- Problematyczna reprezentacja emoji

### gpt2
**Charakterystyka:** Klasyczny tokenizer GPT-2 (BPE), sÅ‚ownik: 50,257 tokenÃ³w

```
Vocab length: 50257

 English  and  CAP ITAL IZ ATION 
 ï¿½ ï¿½ ï¿½  ï¿½ ï¿½ ï¿½    ï¿½ ï¿½ ï¿½ 
 show _ t ok ens  False  None  el if  ==  >=  else :  two  tabs :"        "  Three  tabs :  "              " 
 12 . 0 * 50 = 600 
 Pr zyk Å‚ adow e  z dan ie  w  j ï¿½ ï¿½ zy ku  pol sk im ,  ï¿½ ï¿½ Ã³ Å‚ Ä‡ 
```
**Obserwacje:**
- ZnaczÄ…ca degradacja reprezentacji znakÃ³w Unicode
- Nieprecyzyjna obsÅ‚uga sekwencji biaÅ‚ych znakÃ³w
- Brak obsÅ‚ugi czÄ™Å›ci polskich znakÃ³w

### google/flan-t5-small
**Charakterystyka:** Kompaktowy model T5 (Text-to-Text Transfer Transformer) z instrukcyjnym fine-tuningiem, sÅ‚ownik: 32,100 tokenÃ³w
```
Vocab length: 32100
English and CA PI TAL IZ ATION  <unk>  <unk>  <unk> show _ to ken s Fal s e None  e l if = = > = else : two tab s : " " Three tab s : " " 12. 0 * 50 = 600 Pr zy k <unk> a dow e  z d ani e  w  j <unk> zy ku  pol s kim ,  <unk> Ã³ <unk>  </s>
```
**Obserwacje:**
- Token `</s>` jako marker koÅ„ca sekwencji (charakterystyczny dla T5)
- `<unk>` dla znakÃ³w spoza sÅ‚ownika
- Ograniczona efektywnoÅ›Ä‡ dla tekstÃ³w wielojÄ™zycznych

### BigCode StarCoder2-15B

**Charakterystyka:** Specjalizowany model dla generacji kodu, sÅ‚ownik: 49,152 tokeny
```
Vocab length: 49152

 English  and  CAPITAL IZATION 
 ï¿½ ï¿½ ï¿½  ï¿½ ï¿½ ï¿½     ï¿½ ï¿½ 
 show _ tokens  False  None  elif  ==  >=  else :  two  tabs :"      "  Three  tabs :  "         " 
 1 2 . 0 * 5 0 = 6 0 0 
 Pr zy k Å‚ adow e  z d anie  w  j Ä™ zy ku  pol sk im ,  Å¼ Ã³ Å‚ Ä‡ 
```

**Obserwacje:**
- Precyzyjna obsÅ‚uga skÅ‚adni programistycznej (operatory, sÅ‚owa kluczowe)
- Atomizacja cyfr w wyraÅ¼eniach numerycznych
- RozsÄ…dna reprezentacja polskich znakÃ³w diakrytycznych
- Nadal problematyczna obsÅ‚uga emoji

### xlm-roberta-large
**Charakterystyka:** WielojÄ™zyczny model Transformer, sÅ‚ownik: 250,002 tokeny

```
Vocab length: 250002
<s> English and CAP ITA LIZA TION  ğŸµ  <unk>  é¸Ÿ show _ tok ens Fal se No ne el if  == > = else : two tab s : " " Three tab s : " " 1 2.0 * 50 = 600 Przy kÅ‚ad owe z danie w jÄ™zyk u polskim ,  Å¼Ã³Å‚ Ä‡ </s> 
```
**Obserwacje:**
- **Najlepsze wsparcie dla jÄ™zyka polskiego** wÅ›rÃ³d wszystkich testowanych modeli, najpewniej za sprawÄ… najwiÄ™kszego sÅ‚ownika
- Rozpoznawanie emoji muzycznej ğŸµ i chiÅ„skiego znaku é¸Ÿ
- Minimalna fragmentacja sÅ‚Ã³w w jÄ™zyku polskim
- Tokeny `<s>` i `</s>` na poczÄ…tku i koÅ„cu sekwencji.

## Kluczowe obserwacje

| Aspekt | Wnioski |
|--------|---------|
| **Rozmiar sÅ‚ownika** | Od ~29k (BERT) do ~250k (XLM-RoBERTa). WiÄ™kszy sÅ‚ownik = bardziej efektywna tokenizacja i mniej sub-tokenÃ³w |
| **Wsparcie wielojÄ™zyczne** | Silnie zaleÅ¼ne od rozmiaru sÅ‚ownika. MaÅ‚e sÅ‚owniki rozbijÄ… nieznane sÅ‚owa na wiele drobnych tokenÃ³w |
| **ObsÅ‚uga emoji i Unicode** | Modele nowszej generacji (XLM-RoBERTa, GPT-4) radzÄ… sobie znaczÄ…co lepiej |
| **Specjalizacja** | Modele domenowe (StarCoder dla kodu) lepiej obsÅ‚ugujÄ… swojÄ… dziedzinÄ™ |
| **JÄ™zyk polski** | Najlepsza obsÅ‚uga w XLM-RoBERTa dziÄ™ki wielojÄ™zycznemu treningowi i duÅ¼emu sÅ‚ownikowi |

## Co to oznacza w praktyce?

WybÃ³r odpowiedniego tokenizera powinien byÄ‡ uzaleÅ¼niony od konkretnego przypadku uÅ¼ycia:

* **Dla tekstÃ³w angielskich:**
    - WiÄ™kszoÅ›Ä‡ tokenizerÃ³w zapewni dobre rezultaty
    - GPT-4 i XLM-RoBERTa oferujÄ… najlepszÄ… efektywnoÅ›Ä‡
* **Dla generacji kodu:**
    - **StarCoder** - dedykowany, precyzyjny w obsÅ‚udze skÅ‚adni 
    - GPT-4 - uniwersalny, sprawdza siÄ™ rÃ³wnieÅ¼ w kodzie
* **Dla tekstÃ³w wielojÄ™zycznych (w tym polskiego):**
    - **XLM-RoBERTa** - bezkonkurencyjny lider
    - Modele anglojÄ™zyczne (BERT, GPT-2) mogÄ… znaczÄ…co fragmentowaÄ‡ tekst
* **Dla emoji i Unicode:**
    - Nowsze modele (XLM-RoBERTa, GPT-4, Qwen)
    - Unikaj starszych tokenizerÃ³w (GPT-2, wczesne BERT)

## Podsumowanie

Tokenizer to czÄ™sto niedoceniany, ale kluczowy element kaÅ¼dego LLM. Jak pokazujÄ… powyÅ¼sze porÃ³wnania, rÃ³Å¼nice miÄ™dzy tokenizerami mogÄ… byÄ‡ znaczÄ…ce - szczegÃ³lnie przy pracy z jÄ™zykami innymi niÅ¼ angielski, znakami specjalnymi czy kodem ÅºrÃ³dÅ‚owym.
WybÃ³r tokenizera ma bezpoÅ›redni wpÅ‚yw na:
- **EfektywnoÅ›Ä‡** - mniej tokenÃ³w = szybsze przetwarzanie i niÅ¼sze koszty API
- **JakoÅ›Ä‡** - lepsza reprezentacja = lepsze zrozumienie kontekstu przez model
- **UniwersalnoÅ›Ä‡** - wsparcie dla rÃ³Å¼nych jÄ™zykÃ³w i formatÃ³w tekstu
Warto eksperymentowaÄ‡ z rÃ³Å¼nymi modelami i tokenizerami, aby znaleÅºÄ‡ optymalne rozwiÄ…zanie dla swojego przypadku uÅ¼ycia.
A no i ostatnia uwaga na marginesie, emoji ğŸ¥¸ jest stosunkowo nowe (Unicode 13.0, 2020) wiÄ™c najprawdopodobniej dlatego Å¼aden z tokenizerÃ³w go poprawnie nie rozpoznaÅ‚

## Przydatne linki

- [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/index)
- [Tokenizer Arena - interaktywne porÃ³wnanie](https://huggingface.co/spaces/Xenova/the-tokenizer-playground)
- [Kurs: How Transformer LLMs Work](https://www.deeplearning.ai/short-courses/how-transformer-llms-work/)
- [OpenAI Tokenizer](https://platform.openai.com/tokenizer)


## EN