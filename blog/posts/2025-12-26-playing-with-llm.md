---
title_pl: ""
title_en: ""
date: 2025-12-26
author: "Marcin Piotrowski"
tags: []
description_pl: ""
description_en: ""
---

## PL

## WstÄ™p

> Niniejszy wpis zostaÅ‚ w duÅ¼ej mierze oparty na wiedzy zdobytej w krÃ³tkim darmowym kursie dostÄ™pnym na platformie deeplearning.ai pod linkiem: https://www.deeplearning.ai/short-courses/how-transformer-llms-work/

## Tokenizer

Tokenizer stanowi punkt wejÅ›cia do kaÅ¼dego duÅ¼ego modelu jÄ™zykowego. MoÅ¼na powiedzieÄ‡, Å¼e stanowi most pomiÄ™dzy czÅ‚owiekiem a modelem, poniewaÅ¼ model nie operuje bezpoÅ›rednio na sÅ‚owach czy literach, lecz na tokenach. W praktyce czÄ™sto upraszcza siÄ™, Å¼e sÅ‚owo = token, lecz w rzeczywistoÅ›ci jedno sÅ‚owo moÅ¼e skÅ‚adaÄ‡ siÄ™ z wielu tokenÃ³w. KaÅ¼dy LLM posiada swÃ³j wÅ‚asny sÅ‚ownik tokenÃ³w - kaÅ¼dy token ma unikalne ID. Zadaniem tokenizera jest zamiana tekstu na ciÄ…g tokenÃ³w i przekazanie listy ich ID, aby model mÃ³gÅ‚ wykonaÄ‡ swojÄ… pracÄ™. W tym wpisie przybliÅ¼Ä™ dziaÅ‚anie rÃ³Å¼nych tokenizerÃ³w w praktyce i zaobserwujemy rÃ³Å¼nice miÄ™dzy nimi, nie wchodzÄ…c w szczegÃ³Å‚y techniczne. Wykorzystamy do tego celu API Hugging Face.

```python
from transformers import AutoTokenizer
sentence = "Hello world!"
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokens = tokenizer(sentence)
```

`tokens` to obiekt typu BatchEncoding (dziaÅ‚a jak sÅ‚ownik).
Zawiera przetworzone dane wejÅ›ciowe dla modelu:

* input_ids - tekst zamieniony na liczby (ID tokenÃ³w)
*  attention_mask - ktÃ³re pozycje sÄ… prawdziwymi tokenami (1), a ktÃ³re paddingiem (0)
*  token_type_ids - rozrÃ³Å¼nienie zdaÅ„ w parach zdaÅ„

```
{'input_ids': [101, 8667, 1362, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}

```

Aby zdekodowaÄ‡ id tokenÃ³w, do konkretnych sÅ‚Ã³w wystarczy uÅ¼yÄ‡ funkcji `decode`

```python
for id in token_ids:
    print(tokenizer.decode(id))
```
```
[CLS]
Hello
world
!
[SEP]
```

PowyÅ¼szy przykÅ‚ad demonstruje operacje wykonywane przez kaÅ¼dy LLM podczas obsÅ‚ugi naszych zapytaÅ„. Najpierw wejÅ›ciowy prompt jest zamieniany na tokeny, z kolei na sam koniec tokeny sÄ… z powrotem dekodowane do tekst aby uÅ¼ytkownik mÃ³gÅ‚ go przeczytaÄ‡.

Istnieje wiele tokenizerÃ³w gdzie kaÅ¼dy wykonuje swojÄ… pracÄ™ w inny sposÃ³b, poniÅ¼ej porÃ³wnanie kilku z nich. Tekst poniÅ¼ej posÅ‚uÅ¼y do testowania rÃ³Å¼nych tokenizerÃ³w. Zawiera potencjalne puÅ‚apki takie jak emoji, wielkie litery, fragmenty kodu, biaÅ‚e znaki, liczby oraz zdanie w jÄ™zyku polskim. Pozwoli to zaobserwowaÄ‡ rÃ³Å¼nice w dziaÅ‚aniu poszczegÃ³lnych tokenizerÃ³w.

```python
text = """
English and CAPITALIZATION
ðŸŽµ ðŸ¥¸  é¸Ÿ
show_tokens False None elif == >= else: two tabs:"    " Three tabs: "       "
12.0*50=600
PrzykÅ‚adowe zdanie w jÄ™zyku polskim, Å¼Ã³Å‚Ä‡
"""
```

### bert-base-cased 

```
Vocab length: 28996
[CLS] English and CA ##PI ##TA ##L ##I ##Z ##AT ##ION [UNK] [UNK] [UNK] show _ token ##s F ##als ##e None el ##if = = > = else : two ta ##bs : " " Three ta ##bs : " " 12 . 0 * 50 = 600 P ##rz ##yk ##Å‚a ##do ##we z ##dan ##ie w j ##Ä™ ##zy ##ku p ##ols ##kim , Å¼ ##Ã³ ##Å‚ ##Ä‡ [SEP]
```

### bert-base-uncased

```
ocab length: 30522
[CLS] english and capital ##ization [UNK] [UNK] [UNK] show _ token ##s false none eli ##f = = > = else : two tab ##s : " " three tab ##s : " " 12 . 0 * 50 = 600 pr ##zy ##k ##Å‚a ##do ##we z ##dan ##ie w je ##zy ##ku pol ##ski ##m , z ##o ##Å‚ ##c [SEP]
```

### Xenova/gpt-4

```
Vocab length: 100263

 English  and  CAPITAL IZATION 
 ï¿½ ï¿½ ï¿½  ï¿½ ï¿½ ï¿½    ï¿½ ï¿½ ï¿½ 
 show _tokens  False  None  elif  ==  >=  else :  two  tabs :"      "  Three  tabs :  "         "
 12 . 0 * 50 = 600 
 Pr zy k Å‚ adow e  zd anie  w  j Ä™ zy ku  pol sk im ,  Å¼ Ã³Å‚ Ä‡ 
```

### gpt2

```
Vocab length: 50257

 English  and  CAP ITAL IZ ATION 
 ï¿½ ï¿½ ï¿½  ï¿½ ï¿½ ï¿½    ï¿½ ï¿½ ï¿½ 
 show _ t ok ens  False  None  el if  ==  >=  else :  two  tabs :"        "  Three  tabs :  "              " 
 12 . 0 * 50 = 600 
 Pr zyk Å‚ adow e  z dan ie  w  j ï¿½ ï¿½ zy ku  pol sk im ,  ï¿½ ï¿½ Ã³ Å‚ Ä‡ 
```

### google/flan-t5-small

```
Vocab length: 32100
English and CA PI TAL IZ ATION  <unk>  <unk>  <unk> show _ to ken s Fal s e None  e l if = = > = else : two tab s : " " Three tab s : " " 12. 0 * 50 = 600 Pr zy k <unk> a dow e  z d ani e  w  j <unk> zy ku  pol s kim ,  <unk> Ã³ <unk>  </s>
```

### bigcode/starcoder2-15b

```
Vocab length: 49152

 English  and  CAPITAL IZATION 
 ï¿½ ï¿½ ï¿½  ï¿½ ï¿½ ï¿½     ï¿½ ï¿½ 
 show _ tokens  False  None  elif  ==  >=  else :  two  tabs :"      "  Three  tabs :  "         " 
 1 2 . 0 * 5 0 = 6 0 0 
 Pr zy k Å‚ adow e  z d anie  w  j Ä™ zy ku  pol sk im ,  Å¼ Ã³ Å‚ Ä‡ 
```

### microsoft/Phi-3-mini-4k-instruct

```
Vocab length: 32011
 
 English and C AP IT AL IZ ATION 
 ï¿½ ï¿½ ï¿½ ï¿½  ï¿½ ï¿½ ï¿½ ï¿½   ï¿½ ï¿½ ï¿½ 
 show _ to kens False None elif == >= else : two tabs :"    " Three tabs : "       " 
 1 2 . 0 * 5 0 = 6 0 0 
 Pr zyk Å‚ad owe zd anie w j Ä™ zy ku pol skim , Å¼ Ã³Å‚ Ä‡ 
```

### Qwen/Qwen2-VL-7B-Instruct

```
Vocab length: 151657

 English  and  CAPITAL IZATION 
 ðŸŽµ  ï¿½ ï¿½ ï¿½    ï¿½ ï¿½ ï¿½ 
 show _tokens  False  None  elif  ==  >=  else :  two  tabs :"      "  Three  tabs :  "         "
 1 2 . 0 * 5 0 = 6 0 0 
 Pr zy k Å‚ adow e  zd anie  w  jÄ™zy ku  pol sk im ,  Å¼ Ã³Å‚ Ä‡ 
```

### xlm-roberta-large

```
Vocab length: 250002
<s> English and CAP ITA LIZA TION  ðŸŽµ  <unk>  é¸Ÿ show _ tok ens Fal se No ne el if  == > = else : two tab s : " " Three tab s : " " 1 2.0 * 50 = 600 Przy kÅ‚ad owe z danie w jÄ™zyk u polskim ,  Å¼Ã³Å‚ Ä‡ </s> 
```

## EN